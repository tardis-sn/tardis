{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a382a47",
   "metadata": {},
   "source": [
    "# Convert atomic data\n",
    "\n",
    "This notebook demonstrates how a specific atomic data file is converted from an old continuum TARDIS version to modern TARDIS so the two can be compared directly without re-generating atomic data for a deprecated and unsupported atom data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a629c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pickle\n",
    "import platform\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9df3f",
   "metadata": {},
   "source": [
    "## Utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e69a3",
   "metadata": {},
   "source": [
    "\n",
    "The basic procedure here is to convert a structured array into a `pandas` dataframe matching the structure of our template atomic data.\n",
    "\n",
    "To accomplish this, the structures we need to create from the old data format are an `pd.Index` object to organize the rows, columns with datatypes matching the template, and the table data itself.\n",
    "\n",
    "Most of these table conversions can be reduced to two basic operations - porting a table that has a multi-index, and porting a table that has a simple index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fe7e2",
   "metadata": {},
   "source": [
    "### Multi-Index Port\n",
    "\n",
    "Tables with a multi-index in the new format are missing the formal `pd.MultiIndex` structure, but contain all the same indices in the same format. These are fairly straightforward to convert.\n",
    "\n",
    "The only trick is making sure the dtypes of the ported data columns match the format of the template atom data. After the `pd.DataFrame` is created from the old data, we loop through each column and set its dtype to be equal to that of the template.\n",
    "\n",
    "The same procedure is applied to each level of the `pd.MultiIndex`, as these are converted to floats by default, but we typically want `int` for atomic number, levels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiindex_port(olddata, templatedata, templatekey, oldkey=None):\n",
    "    \"\"\"\n",
    "    Convert an structured-array dataset into a pandas.DataFrame that matches the\n",
    "    index names and dtypes of a provided template.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    olddata : h5py.File object\n",
    "        Structured array(s) from the data to be converted. The structured array is\n",
    "        accessed as olddata[oldkey].\n",
    "    templatedata : pandas.DataFrame\n",
    "        Container providing a template pandas object (DataFrame or Series) whose\n",
    "        index names and column/element dtypes should be matched.\n",
    "    templatekey : hashable\n",
    "        Key to select the template object from templatedata.\n",
    "    oldkey : hashable, optional\n",
    "        Key to select the old structured array from olddata. If None, templatekey\n",
    "        is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame constructed from the old structured array with its index set to\n",
    "        the template's index names, column dtypes coerced to the template dtypes,\n",
    "        and index-level dtypes converted to match the template.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "        If templatekey or oldkey is not present in the provided containers.\n",
    "    TypeError, ValueError\n",
    "        If coercion to the template dtypes or index conversions fail.\n",
    "    \"\"\"\n",
    "    if oldkey is None:\n",
    "        oldkey = templatekey\n",
    "\n",
    "    # Get the format of the multi-index from the desired template\n",
    "    index_names = templatedata[templatekey].index.names\n",
    "\n",
    "    # Attempt conversion of the old structured array to a pd DataFrame\n",
    "    newdata = pd.DataFrame(olddata[oldkey][:]).set_index(index_names)\n",
    "\n",
    "    # Check datatypes of columns, convert if necessary\n",
    "    if hasattr(templatedata[templatekey], \"columns\"):\n",
    "        # Handle multiple columns\n",
    "        for col in templatedata[templatekey].columns:\n",
    "            desired_dtype = templatedata[templatekey][col].dtype\n",
    "            if desired_dtype == np.dtype(\"object\"):\n",
    "                desired_dtype = str\n",
    "            newdata[col] = newdata[col].astype(desired_dtype)\n",
    "    else:\n",
    "        # Handle single columns - in this case, we don't need to loop over multiple\n",
    "        # column names or data types\n",
    "        col = [templatedata[templatekey].name]\n",
    "        newdata[col] = newdata[col].astype(templatedata[templatekey].dtype)\n",
    "\n",
    "    # Convert datatypes of each level of the multi-index\n",
    "    if isinstance(newdata.index, pd.Index):\n",
    "        # Handle single index object\n",
    "        template_ind_dtype = templatedata[templatekey].index.dtype\n",
    "        newdata.index = newdata.index.astype(template_ind_dtype)\n",
    "    elif isinstance(newdata.index, pd.MultiIndex):\n",
    "        # Handle multi-index - need to loop over each index level\n",
    "        for i, indname in enumerate(newdata.index.names):\n",
    "            template_ind_dtype = templatedata[templatekey].index.dtypes[indname]\n",
    "            converted_index = newdata.index.levels[i].astype(template_ind_dtype)\n",
    "            newdata.index = newdata.index.set_levels(converted_index, level=i)\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2ea5e",
   "metadata": {},
   "source": [
    "### Simple Port\n",
    "\n",
    "For tables that only have a single index, the conversion is much simpler. We just need to create the `pd.DataFrame` from the old structured array and then ensure the dtypes of the columns and (single) index match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_port(olddata, templatedata, templatekey, oldkey=None):\n",
    "    \"\"\"\n",
    "    Convert an old structured-array dataset into a pandas.DataFrame that matches the\n",
    "    column names and dtypes of a provided template DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    olddata : h5py.File object\n",
    "        Container holding the old structured array(s). The structured array is read\n",
    "        as olddata[oldkey].\n",
    "    templatedata : pandas.DataFrame\n",
    "        Container providing a template DataFrame whose column dtypes should be matched.\n",
    "    templatekey : hashable\n",
    "        Key to select the template object from templatedata.\n",
    "    oldkey : hashable, optional\n",
    "        Key to select the old structured array from olddata. If None, templatekey\n",
    "        is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame constructed from the old structured array with column dtypes\n",
    "        coerced to match the template's dtypes.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "        If templatekey or oldkey is not present in the provided containers.\n",
    "    TypeError, ValueError\n",
    "        If coercion to the template dtypes fails.\n",
    "    \"\"\"\n",
    "    if oldkey is None:\n",
    "        oldkey = templatekey\n",
    "\n",
    "    newdata = pd.DataFrame(olddata[oldkey][:])\n",
    "\n",
    "    # Check datatypes of columns, convert if necessary\n",
    "    for col in templatedata[templatekey].columns:\n",
    "        desired_dtype = templatedata[templatekey][col].dtype\n",
    "        if desired_dtype == np.dtype(\"object\"):\n",
    "            desired_dtype = str\n",
    "        newdata[col] = newdata[col].astype(desired_dtype)\n",
    "\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccce523",
   "metadata": {},
   "source": [
    "### Checksum Utility Functions\n",
    "\n",
    "These are included directly from the `carsus` utilities for writing checksum metadata for an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_pandas_object(pd_object):\n",
    "    \"\"\"\n",
    "    Serialize Pandas objects with Pickle.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pd_object : pandas.Series or pandas.DataFrame\n",
    "        Pandas object to be serialized with Pickle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pickle serialized Python object.\n",
    "    \"\"\"\n",
    "    return pickle.dumps(pd_object)\n",
    "\n",
    "\n",
    "def hash_pandas_object(pd_object, algorithm=\"md5\"):\n",
    "    \"\"\"\n",
    "    Hash Pandas objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pd_object : pandas.Series or pandas.DataFrame\n",
    "        Pandas object to be hashed.\n",
    "    algorithm : str, optional\n",
    "        Algorithm available in `hashlib`, by default \"md5\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Hash values.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `algorithm` is not available in `hashlib`.\n",
    "    \"\"\"\n",
    "    algorithm = algorithm.lower()\n",
    "\n",
    "    if hasattr(hashlib, algorithm):\n",
    "        hash_func = getattr(hashlib, algorithm)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('algorithm not supported')\n",
    "\n",
    "    return hash_func(serialize_pandas_object(pd_object)).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d71e9",
   "metadata": {},
   "source": [
    "## Converting the atomic data\n",
    "\n",
    "The utility functions above are included for completeness. For the rest of this script, I'll import these functions directly from the Python script in `./tardis/scripts/convert_atomic_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ded02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tardis.scripts import convert_atomic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406b61b",
   "metadata": {},
   "source": [
    "### Loading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a98d2f",
   "metadata": {},
   "source": [
    "We need to use three existing atomic data files to produce our new file in the modern TARDIS format:\n",
    "\n",
    "1. Old atomic data\n",
    "2. Old photoionization data\n",
    "3. A template to use as a reference for what our desired format should be\n",
    "\n",
    "**Be sure to change the file paths below to reflect the directory structure of your system**. By default, I've selected an NLTE atom data file to use as a template for conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40155a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldatomdata_filename = \"merged_mod_20SNG_forbidden_yg_fix_H30_cmfgen_yg.h5\"\n",
    "oldpidata_filename = \"photoionization_data_H30_He.h5\"\n",
    "template_filename = \"/home/connor/tardis-regression-data/atom_data/nlte_atom_data/TestNLTE_He_Ti.h5\"\n",
    "new_filename = \"converted_atom_data.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11408b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open HDFStore file objects for each of the paths defined above\n",
    "# By default, HDFStore will *create* the file if it doesn't exist,\n",
    "# which we don't want for the old files we're trying to convert.\n",
    "if Path(oldatomdata_filename).is_file():\n",
    "    old_df = pd.HDFStore(oldatomdata_filename)\n",
    "else:\n",
    "    raise FileNotFoundError()\n",
    "\n",
    "if Path(oldpidata_filename).is_file():\n",
    "    pi_data = pd.HDFStore(oldpidata_filename)\n",
    "else:\n",
    "    raise FileNotFoundError()\n",
    "\n",
    "if Path(template_filename).is_file():\n",
    "    template = pd.HDFStore(template_filename)\n",
    "else:\n",
    "    raise FileNotFoundError()\n",
    "\n",
    "if Path(new_filename).is_file():\n",
    "    raise FileExistsError(f\"Destination file {new_filename} already exists. Delete it or specify a different destination.\")\n",
    "else:\n",
    "    new = pd.HDFStore(new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435f2ef",
   "metadata": {},
   "source": [
    "### Collisions data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448afa8",
   "metadata": {},
   "source": [
    "Let's take a look at the structure of our data to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380291d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05d726",
   "metadata": {},
   "source": [
    "There's certainly more data in this file, but the only dataframe `pandas` can access from this format is `/Y/g`, which contains the collision data. We can extract and convert this first and then re-load the dataframe with `h5py` directly to get the rest of the information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ed096",
   "metadata": {},
   "source": [
    "The keys above represent the format into which we want to convert the old atom data. We'll start with the collisions data and then proceed through this list.\n",
    "\n",
    "Here we convert the first four columns of the collisions data to a `pd.MultiIndex` to match the template data structure and rename the columns to the index of the temperature bin each column represents.\n",
    "\n",
    "(In the old atomic data format, the temperatures themselves are used as the column headers. In the new format, the *index* of these temperatures is used as the column header and the temperatures themselves are stored in the `collisions_metadata` field.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f046bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLLISIONS DATA\n",
    "multiindex_cols = list(old_df[\"/Y/g\"].columns[:4])\n",
    "new[\"collisions_data\"] = old_df[\"/Y/g\"].set_index(multiindex_cols)\n",
    "tempcols = list(new['collisions_data'].columns)\n",
    "new['collisions_data'] = new['collisions_data'].rename(lambda f: tempcols.index(f), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a00b53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47cd1b93",
   "metadata": {},
   "source": [
    "### Re-load Old Atom Data with h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27018ed",
   "metadata": {},
   "source": [
    "\n",
    "We've gotten all useful info out of the old dataframe using `pandas`, so now load the same file with h5py directly to get the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.close()\n",
    "old = h5py.File(oldatomdata_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0969a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "old.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c9d64",
   "metadata": {},
   "source": [
    "Now we can see the other fields that will populate our new atom data file---we just have to convert them to the same data structures and key names as the template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12799c90",
   "metadata": {},
   "source": [
    "### Collisions Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b3682",
   "metadata": {},
   "source": [
    "This particular atomic data format stores the temperatures for the collisions data table as a separate array stored in a \"metadata\" field. The `AtomData` loader class in TARDIS checks for this field when loading data written in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ad2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tempcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d64816",
   "metadata": {},
   "source": [
    "These are the temperatures associated with the `collisions_data` we loaded in previously - let's insert these values in the existing metadata structure of the template atom data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata = template[\"collisions_metadata\"].copy()\n",
    "new_metadata[\"temperatures\"] = np.array(tempcols, dtype=np.int64)\n",
    "new[\"collisions_metadata\"] = new_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3b474",
   "metadata": {},
   "source": [
    "### Atom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_atom_data = convert_atomic_data.multiindex_port(\n",
    "    old, template, \"atom_data\", oldkey=\"basic_atom_data\"\n",
    ")\n",
    "new[\"atom_data\"] = new_atom_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ef878",
   "metadata": {},
   "source": [
    "### Ionization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"ionization_data\"] = convert_atomic_data.multiindex_port(\n",
    "    old, template, \"ionization_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160f5b6",
   "metadata": {},
   "source": [
    "### Levels Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe4ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"levels_data\"] = convert_atomic_data.multiindex_port(\n",
    "    old, template, \"levels_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da4917",
   "metadata": {},
   "source": [
    "### Lines Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e282bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"lines_data\"] = convert_atomic_data.multiindex_port(\n",
    "    old, template, \"lines_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af018501",
   "metadata": {},
   "source": [
    "### Lines Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(template[\"lines_metadata\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f96f6",
   "metadata": {},
   "source": [
    "This is a super simple field that doesn't exist in the original format, so we can just copy it over for consistency in case a particular atom data reader needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"lines_metadata\"] = template[\"lines_metadata\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f2fee",
   "metadata": {},
   "source": [
    "### Macroatom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76891b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"macro_atom_data\"] = convert_atomic_data.simple_port(old, template, \"macro_atom_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27d923",
   "metadata": {},
   "source": [
    "### Macroatom References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"macro_atom_references\"] = convert_atomic_data.multiindex_port(\n",
    "    old, template, \"macro_atom_references\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb716dba",
   "metadata": {},
   "source": [
    "### Photoionization Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ac540",
   "metadata": {},
   "source": [
    "Note that this data is stored in an entirely different file - in this case, it is already in the correct format for our new atomic data file, so we can just copy it into our new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pi_data['photoionization_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b44e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "new[\"photoionization_data\"] = pi_data[\"photoionization_data\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e19889",
   "metadata": {},
   "source": [
    "### Zeta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09785ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old['zeta_data'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1a7e1",
   "metadata": {},
   "source": [
    "This isn't formatted as a structured array, so the `multiindex_port` function won't be able to grab specific columns by name. We could add this functionality to the porting function, but this is the only field for which this is a problem. We'll just handle this one by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta_index = pd.MultiIndex.from_arrays(\n",
    "    old[\"zeta_data\"][:, :2].T.astype(np.int64),\n",
    "    names=template[\"zeta_data\"].index.names,\n",
    ")\n",
    "zeta_temps = pd.Index(\n",
    "    old[\"zeta_data\"].attrs[\"t_rad\"].astype(np.float64), name=\"temp\"\n",
    ")\n",
    "new_zeta_data = pd.DataFrame(\n",
    "    old[\"zeta_data\"][:, 2:], index=zeta_index, columns=zeta_temps\n",
    ")\n",
    "new[\"zeta_data\"] = new_zeta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82fd20",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied over from Andrew's notebook demonstrating how to do this\n",
    "meta = []\n",
    "meta.append((\"format\", \"version\", \"1.0\"))\n",
    "\n",
    "total_checksum = hashlib.md5()\n",
    "for key in new.keys():\n",
    "   # update the total checksum to sign the file\n",
    "   total_checksum.update(convert_atomic_data.serialize_pandas_object(new[key]))\n",
    "\n",
    "   # save individual DataFrame/Series checksum\n",
    "   checksum = convert_atomic_data.hash_pandas_object(new[key])\n",
    "   meta.append((\"md5sum\", key.lstrip(\"/\"), checksum))\n",
    "\n",
    "# relevant package versions\n",
    "meta.append((\"software\", \"python\", platform.python_version()))\n",
    "imports = [\n",
    "   \"carsus\",\n",
    "   \"astropy\",\n",
    "   \"numpy\",\n",
    "   \"pandas\",\n",
    "   \"tables\",\n",
    "   \"ChiantiPy\",\n",
    "]\n",
    "for package in imports:\n",
    "   meta.append((\"software\", package, __import__(package).__version__))\n",
    "meta_df = pd.DataFrame.from_records(\n",
    "   meta, columns=[\"field\", \"key\", \"value\"], index=[\"field\", \"key\"]\n",
    ")\n",
    "uuid1 = uuid.uuid1().hex\n",
    "new.root._v_attrs[\"MD5\"] = total_checksum.hexdigest()\n",
    "new.root._v_attrs[\"UUID1\"] = uuid1\n",
    "new.root._v_attrs[\"FORMAT_VERSION\"] = \"1.0\"\n",
    "tz = pytz.timezone(\"UTC\")\n",
    "date = datetime.now(tz).isoformat()\n",
    "new.root._v_attrs[\"DATE\"] = date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd802e9",
   "metadata": {},
   "source": [
    "### Write out / Close files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc55c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "old.close()\n",
    "template.close()\n",
    "pi_data.close()\n",
    "new.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
